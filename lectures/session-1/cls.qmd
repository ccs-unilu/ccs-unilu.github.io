---
title: <i class='bi bi-patch-question'></i> Project
subtitle: Behavioral Data Classification
order: 2
sidebar: main
---

## Context

Many cognitive experiments result in datasets where each trial contains features about the stimulus or condition and an outcome such as correct/incorrect. These datasets raise a central question:

- Given the features of a trial, can we predict the participant's response?

This also applies to questionnaires. Questionnaires produce multiple numeric or categorical responses per participant, often forming patterns that are not obvious by simple inspection. Research question we often want to answer is:

- Can we predict or classify participants based on their responses?

Raw behavioral data (e.g., stimulus condition, response time, accuracy, likert responses) provide valuable information but do not automatically reveal the patterns that distinguish one response from another. We need a computational approach that can discover useful structure in the data and make predictions from it.

## Approach

A simple yet powerful solution is to treat this as a classification problem. Using tools from the ML, we can train a model such as logistic regression, decision trees, or SVM to learn the relationship between input features and behavioral outputs.

By training on labeled data (features + labels), the model can learn to classify new trials or participants based on their features alone.


## TODO

Choose a prediction task:
- Predict "high stress vs low stress"
- Predict "introvert vs extrovert"
- Predict "responded correctly vs incorrectly under high load"

### Define X and y
X = questionnaire responses
y = participant category

### Data inspection

- Load your dataset using `pd.read_csv()`
- Display `.head()`, `.info()`, `.describe()`
- Identify which variables are: Numerical, Categorical, Binary labels


### Preprocessing

1. Convert categorical variables using:  
   - `pd.get_dummies()` OR `sklearn.preprocessing.OneHotEncoder`
2. Drop/investigate missing data
3. Convert boolean labels (e.g., `is_correct`) into {0, 1}
4. Use seaborn or pandas to visualize distributions of key variables  


### Feature extraction

Create at least 3 new features, such as:

- `rt_z = (rt âˆ’ mean) / std`  
- Log-transform RTs: `log_rt = np.log(rt)`
- Average Likert score
- Condition difficulty coding (easy=0, hard=1)
- Response variability per participant

### Train a logistic regression

- `from sklearn.linear_model import LogisticRegression`
- Define X (features) and y (labels)
- Split X and y into train/test
- Train the model using X_train, y_train
- Compute accuracy on X_test, y_test
- Print model coefficients


### Train a DecisionTreeClassifier

- Plot the tree structure using `sklearn.tree.plot_tree`
- Identify the most informative features


### Model evaluation
- cross validation: `from sklearn.model_selection import cross_val_score`
- Compute 5-fold cross-validation accuracy.
- Report mean, min, max accuracy.
- Compute confusion matrix, precision, recall, F1. Use `from sklearn.metrics import confusion_matrix, classification_report`


- ROC Curve: `from sklearn.metrics import RocCurveDisplay`
- Plot the ROC curve and compute AUC.

### Model comparison

- Compare Logistic Regression vs Random Forest: Accuracy, ROC AUC, Confusion matrix. Which performs better and why?


### Hyperparameter tuning

- Compare different SVM kernels: linear, rbf, poly: `from sklearn.model_selection import GridSearchCV`


### Improve the model

- Use dimensionality reduction: `from sklearn.decomposition import PCA`
- Apply class imbalance techniques if needed: class weights, SMOTE oversampling, ROC-AUC evaluation
- Feature engineering: aggregate trial-level features into participant-level features. Examples:
   - Mean accuracy per condition
   - RT variability
   - Questionnaire subscale scores
- Train model to classify participants instead of trials.


### Explainable AI

- Permutation importance
- SHAP values
- Partial dependence plots
- Which cognitive features drive the classification?
